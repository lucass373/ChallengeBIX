{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "data = pd.read_csv('air_system_present_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "neg    15625\n",
      "pos      375\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#verificando quantidade de caminhões com defeito\n",
    "df_defeito = data['class'].value_counts()\n",
    "print(df_defeito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['class'])\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituindo 'na' por NaN para representar valores ausentes\n",
    "X.replace('na', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa_000        0\n",
      "ab_000    12363\n",
      "ac_000      926\n",
      "ad_000     3981\n",
      "ae_000      690\n",
      "          ...  \n",
      "ee_007      192\n",
      "ee_008      192\n",
      "ee_009      192\n",
      "ef_000      762\n",
      "eg_000      762\n",
      "Length: 170, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verificar valores ausentes\n",
    "missing_values = X.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking care of missing data\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X)\n",
    "X= imputer.transform(X)\n",
    "X = pd.DataFrame(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0         1       2      3    4    5    6    7    8         9          10         11         12         13       14       15         16   17    18   19       20        21         22         23        24        25   26   27      28   29      30     31   32   33   34   35   36       37         38         39        40   41       42     43      44        45         46         47         48        49       50   51         52        53        54        55        56        57        58        59        60        61         62     63      64      65    66         67       68        69        70             71             72             73             74             75             76            77             78        79        80         81         82         83       84       85       86         87         88         89        90   91     92   93          94        95         96     97     98   99      100       101       102        103        104       105       106       107       108     109     110        111        112      113    114       115       116       117        118       119      120    121  122     123     124        125        126   127    128  129   130        131     132    133  134  135   136      137  138  139  140  141      142      143     144         145        146       147      148        149       150        151      152  153  154         155      156     157       158       159       160       161       162       163        164       165       166     167  168  169\n",
      "0     60.0  0.000000    20.0   12.0  0.0  0.0  0.0  0.0  0.0    2682.0     4736.0     3862.0     1846.0        0.0      0.0      0.0     3976.0  0.0   0.0  0.0   1520.0    2374.0    11516.0     9480.0  111258.0     470.0  0.0  0.0     0.0  0.0    58.0   26.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0   13124.0  2.0   1956.0  434.0    76.0     174.0      190.0     6198.0     1148.0    2948.0      2.0  0.0     8762.0    2566.0     480.0     380.0     196.0     516.0      86.0      66.0      74.0       0.0   124340.0    4.0     6.0     4.0   0.0     3976.0    318.0  107662.0    3458.0  280200.504745  320483.011169  400614.715805  457347.105202  503282.157837  538685.709058  567845.94899  587884.465343  110980.0     59.53   124340.0   124340.0   184970.0     54.0    686.0  12246.0   123880.0    13196.0  1209600.0     598.0  0.0    6.0  0.0     5913.60      0.00    5851.20    6.0   30.0  0.0     2.0    5512.0    4674.0     1388.0     1508.0      38.0       4.0       0.0       0.0     8.0    12.0   124340.0   0.000000   1550.0   14.0      36.0      26.0     920.0      430.0    7650.0   2294.0  206.0  0.0    22.0    42.0     5336.0     1276.0   0.0    0.0  0.0   0.0     6598.0    70.0  112.0  0.0  0.0   0.0      0.0  0.0  0.0  0.0  0.0    340.0      0.0     0.0      1100.0      574.0     232.0     66.0      780.0     882.0        0.0      4.0  0.0  0.0         0.0   465.50    90.0    7502.0    3156.0    1098.0     138.0     412.0     654.0       78.0      88.0       0.0     0.0  0.0  0.0\n",
      "1     82.0  0.000000    68.0   40.0  0.0  0.0  0.0  0.0  0.0       0.0      748.0    12594.0     3636.0        0.0      0.0      0.0     5244.0  0.0  60.0  0.0      0.0       0.0    23174.0    18166.0   23686.0    1270.0  0.0  0.0     0.0  0.0    12.0   82.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0      692.0   16286.0  0.0    280.0   44.0    50.0    1274.0      866.0     3362.0    11102.0       0.0      0.0  0.0    12564.0    1756.0     638.0     276.0     172.0     132.0     812.0     308.0     192.0     128.0    46894.0    4.0     2.0    38.0   0.0     5244.0    360.0   20520.0    3134.0  280200.504745  320483.011169  400614.715805  457347.105202  503282.157837  538685.709058  567845.94899  587884.465343   23320.0     81.89    46894.0    46894.0    48324.0     68.0      0.0   4486.0    46480.0    17050.0  1209600.0     726.0  2.0    4.0  0.0     7224.96      0.00    7768.32    0.0   42.0  0.0     0.0       4.0    7064.0     6200.0     2452.0    1246.0      12.0       0.0       0.0    14.0    54.0    46894.0   0.000000   2202.0   28.0     114.0     350.0     700.0     1708.0    9622.0   2174.0   80.0  0.0    80.0   206.0     7802.0     1466.0   0.0    0.0  0.0   0.0     7918.0    78.0   40.0  0.0  0.0   0.0      0.0  0.0  0.0  0.0  0.0    352.0      0.0     0.0      3996.0      584.0     200.0     62.0    37580.0    3756.0     6368.0     36.0  0.0  0.0         0.0     2.86   102.0   10040.0    3310.0    1068.0     276.0    1620.0     116.0       86.0     462.0       0.0     0.0  0.0  0.0\n",
      "2  66002.0  2.000000   212.0  112.0  0.0  0.0  0.0  0.0  0.0  199486.0  1358536.0  1952422.0   452706.0    25130.0    520.0      0.0  1891670.0  0.0   0.0  0.0  77898.0  110548.0  3605894.0  3291610.0  959756.0  286536.0  0.0  0.0     0.0  0.0   106.0  340.0  0.0  0.0  0.0  0.0  0.0  24286.0   681260.0  2808472.0  474782.0  0.0  22984.0  894.0  1162.0    1304.0     1856.0   388700.0  3424812.0  123828.0  23260.0  0.0  1662580.0  667960.0  387998.0  226078.0  161558.0  148288.0  154908.0  220820.0  147744.0  210866.0  4644422.0  958.0  7848.0  3624.0   0.0  1891670.0  47066.0  692800.0  265492.0  336240.000000  194360.000000  245240.000000  457347.105202  503282.157837  538685.709058  567845.94899  587884.465343  175480.0  66002.89  4644422.0  4644422.0  4608738.0  22706.0   2272.0  95510.0  1006260.0  3991552.0  1209600.0  126310.0  0.0  104.0  0.0  3594885.12      0.00  374649.60  148.0  720.0  0.0  5154.0  174956.0  622312.0  1002504.0   793938.0  541734.0  345896.0  300806.0  201500.0    52.0    92.0  4644422.0   0.000000  24818.0  964.0  162660.0  215004.0  217930.0  3038612.0  322718.0   6080.0   14.0  0.0   226.0   572.0  3593728.0  1911060.0   0.0  284.0  0.0   0.0  3613906.0  4218.0  692.0  0.0  0.0   0.0      0.0  0.0  0.0  0.0  0.0  25278.0   9438.0  2504.0  10262714.0  1278664.0  109700.0  19072.0     9520.0    4902.0  4434614.0  70900.0  0.0  0.0  26002880.0  2057.84  2158.0  396312.0  538136.0  495076.0  380368.0  440134.0  269556.0  1315022.0  153680.0     516.0     0.0  0.0  0.0\n",
      "3  59816.0  0.772065  1010.0  936.0  0.0  0.0  0.0  0.0  0.0       0.0   123922.0   984314.0  1680050.0  1135268.0  92606.0  14038.0  1772828.0  0.0   0.0  0.0   1116.0    2372.0  3546760.0  3053176.0  652616.0  423374.0  0.0  0.0  7274.0  0.0  1622.0  432.0  0.0  0.0  0.0  0.0  0.0   6388.0  1091104.0  2930694.0    2012.0  0.0   3526.0  904.0  1426.0  223226.0  2663348.0  1137664.0      104.0       0.0      0.0  0.0  1283806.0  928212.0  345132.0  265930.0  194770.0  158262.0  219942.0  620264.0   13880.0       0.0  4201350.0   98.0   238.0   880.0  16.0  1772828.0  51468.0  331744.0  316130.0  176000.000000  208420.000000  159380.000000  457347.105202  503282.157837  538685.709058  567845.94899  587884.465343  100120.0  59816.46  4201350.0  4201350.0  4203050.0  29967.0  26214.0  51894.0   562680.0  4030198.0  1209600.0  114684.0  0.0  144.0  0.0  3387773.76  38633.28  599624.64    0.0    0.0  0.0     0.0   14308.0  475410.0  1109740.0  1528024.0  837114.0   58942.0    6220.0     440.0  1278.0  1292.0  4201350.0  50.023096   6846.0  810.0   70090.0  345884.0  191284.0  2454600.0  926846.0  33558.0  280.0  0.0  1516.0  1398.0  2050280.0    64066.0   0.0  674.0  0.0  46.0  3413978.0  2924.0  414.0  0.0  0.0  60.0  38710.0  0.0  0.0  0.0  0.0  27740.0  33354.0  6330.0         0.0        0.0  133542.0  21290.0  2718360.0  435370.0        0.0      0.0  0.0  0.0   1179900.0  1541.32  1678.0  659550.0  691580.0  540820.0  243270.0  483302.0  485332.0   431376.0  210074.0  281662.0  3232.0  0.0  0.0\n",
      "4   1814.0  0.772065   156.0  140.0  0.0  0.0  0.0  0.0  0.0       0.0       72.0    17926.0    82834.0     3114.0      0.0      0.0    48978.0  0.0   0.0  0.0      0.0       0.0    97146.0    89920.0   12932.0    5092.0  0.0  0.0     0.0  0.0   102.0   50.0  0.0  0.0  0.0  0.0  0.0  11544.0    73570.0     3662.0   15170.0  0.0    126.0   34.0    60.0      58.0    10768.0    92898.0        2.0       0.0      0.0  0.0    53558.0   34620.0    9824.0    4552.0     764.0     294.0     192.0     142.0       0.0       0.0   110094.0    4.0     4.0   986.0   0.0    48978.0   1100.0    7932.0    4966.0  280200.504745  320483.011169  400614.715805  457347.105202  503282.157837  538685.709058  567845.94899  587884.465343   12700.0   1813.74   110094.0   110094.0   113274.0    364.0    330.0  10874.0   109880.0   103946.0  1209600.0    1832.0  0.0    8.0  0.0    94319.04      0.00    9556.80    0.0    0.0  0.0     0.0       0.0    6560.0    77394.0    17810.0    1924.0     230.0      28.0       0.0     2.0    10.0   110094.0  50.023096    144.0   14.0      86.0    5532.0    1792.0    30864.0   63436.0   1986.0   92.0  0.0   230.0   178.0    93820.0    57312.0  44.0    4.0  0.0   0.0    95372.0    78.0   36.0  0.0  0.0   0.0      0.0  0.0  0.0  0.0  0.0    538.0    360.0   142.0         0.0        0.0    1822.0    344.0     2140.0     394.0    13664.0    110.0  0.0  0.0    813740.0   113.86    52.0   10216.0    9958.0    7646.0    4144.0   18466.0   49782.0     3176.0     482.0      76.0     0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# Visualizar as primeiras linhas do conjunto de dados com os valores nulos tratados\n",
    "# \n",
    "print(X.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        neg\n",
      "1        neg\n",
      "2        neg\n",
      "3        neg\n",
      "4        neg\n",
      "        ... \n",
      "15995    neg\n",
      "15996    neg\n",
      "15997    neg\n",
      "15998    neg\n",
      "15999    neg\n",
      "Name: class, Length: 16000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Visualizar alvo\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the Dependent Variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    15625\n",
       "1    15625\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balanceando as classes com SMOTE\n",
    "smote = SMOTE()\n",
    "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "pd.DataFrame(y_balanced).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0          1             2            3           4    \\\n",
      "17595  7.612265e+05   0.772065  3.580393e+08   439.498794    0.000000   \n",
      "10368  5.969800e+04   0.000000  2.130706e+09    84.000000  230.000000   \n",
      "26520  2.901470e+05   0.000000  1.426051e+03  1083.736524    0.000000   \n",
      "17539  8.988779e+05   0.772065  0.000000e+00   439.498794    0.000000   \n",
      "10610  8.600000e+01   0.000000  8.000000e+00     8.000000    2.000000   \n",
      "...             ...        ...           ...          ...         ...   \n",
      "29802  1.639210e+06  10.033759  3.580393e+08   414.111078    6.428347   \n",
      "5390   4.980000e+02   0.772065  4.200000e+01    26.000000    0.000000   \n",
      "860    1.341620e+05   0.000000  3.460000e+02   302.000000    0.000000   \n",
      "15795  8.000000e+00   0.000000  0.000000e+00     0.000000    0.000000   \n",
      "23654  7.800477e+05   0.772065  3.580393e+08   439.498794    2.327048   \n",
      "\n",
      "              5            6              7             8             9    \\\n",
      "17595    0.000000     0.000000       0.000000  0.000000e+00  4.711674e+03   \n",
      "10368  310.000000     0.000000       0.000000  0.000000e+00  0.000000e+00   \n",
      "26520    0.000000     0.000000    5772.221563  8.326847e+04  4.564527e+05   \n",
      "17539    0.000000     0.000000       0.000000  0.000000e+00  1.135829e+04   \n",
      "10610    2.000000     0.000000       0.000000  0.000000e+00  0.000000e+00   \n",
      "...           ...          ...            ...           ...           ...   \n",
      "29802   10.195950  1827.933163  320907.228874  2.437845e+06  7.574225e+06   \n",
      "5390     0.000000     0.000000       0.000000  0.000000e+00  5.000000e+02   \n",
      "860      0.000000     0.000000       0.000000  0.000000e+00  0.000000e+00   \n",
      "15795    0.000000     0.000000       0.000000  0.000000e+00  0.000000e+00   \n",
      "23654    3.690912     0.000000   47463.731825  1.246876e+06  8.668712e+06   \n",
      "\n",
      "       ...           160           161           162           163  \\\n",
      "17595  ...  8.490284e+06  4.119939e+06  9.514286e+06  6.657233e+06   \n",
      "10368  ...  3.318640e+05  1.959720e+05  3.161200e+05  3.100320e+05   \n",
      "26520  ...  3.119124e+05  1.328134e+05  2.509737e+05  2.310863e+05   \n",
      "17539  ...  1.004062e+07  5.481601e+06  1.017126e+07  6.328693e+06   \n",
      "10610  ...  1.476000e+03  1.216000e+03  1.024000e+03  1.420000e+02   \n",
      "...    ...           ...           ...           ...           ...   \n",
      "29802  ...  3.578875e+06  1.354529e+06  2.941144e+06  1.049158e+07   \n",
      "5390   ...  1.174000e+03  5.160000e+02  1.014000e+03  1.268000e+03   \n",
      "860    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "15795  ...  4.200000e+01  8.000000e+00  2.200000e+01  1.800000e+01   \n",
      "23654  ...  3.066263e+06  1.455867e+06  3.263126e+06  7.614925e+06   \n",
      "\n",
      "                164           165           166            167       168  \\\n",
      "17595  3.463890e+06  1.690055e+06  1.033057e+06    4652.350969  0.000000   \n",
      "10368  8.184240e+05  6.275760e+05  5.656000e+03       0.000000  0.000000   \n",
      "26520  2.744540e+05  1.904093e+05  1.091555e+04       0.000000  0.000000   \n",
      "17539  3.854276e+06  2.063534e+06  3.320290e+06  293112.413593  0.000000   \n",
      "10610  2.620000e+02  0.000000e+00  0.000000e+00       0.000000  0.000000   \n",
      "...             ...           ...           ...            ...       ...   \n",
      "29802  2.587151e+06  3.412931e+05  1.737846e+04       0.000000  0.064969   \n",
      "5390   1.214000e+03  1.509600e+04  0.000000e+00       0.000000  0.000000   \n",
      "860    0.000000e+00  0.000000e+00  0.000000e+00       0.000000  0.000000   \n",
      "15795  3.600000e+01  3.600000e+01  0.000000e+00       0.000000  0.000000   \n",
      "23654  7.969561e+06  5.478814e+06  1.440115e+05      10.000000  0.064969   \n",
      "\n",
      "            169  \n",
      "17595  0.000000  \n",
      "10368  6.000000  \n",
      "26520  0.000000  \n",
      "17539  0.000000  \n",
      "10610  0.000000  \n",
      "...         ...  \n",
      "29802  0.242683  \n",
      "5390   0.000000  \n",
      "860    0.000000  \n",
      "15795  0.000000  \n",
      "23654  0.242683  \n",
      "\n",
      "[25000 rows x 170 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os modelos\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=2000),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando e avaliando o modelo: Random Forest\n",
      "Acurácia: 0.99\n",
      "Precisão: 0.99\n",
      "Recall: 1.00\n",
      "F1-Score: 0.99\n",
      "\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      3180\n",
      "           1       0.99      1.00      0.99      3070\n",
      "\n",
      "    accuracy                           0.99      6250\n",
      "   macro avg       0.99      0.99      0.99      6250\n",
      "weighted avg       0.99      0.99      0.99      6250\n",
      "\n",
      "============================================================\n",
      "Treinando e avaliando o modelo: Decision Tree\n",
      "Acurácia: 0.99\n",
      "Precisão: 0.99\n",
      "Recall: 0.99\n",
      "F1-Score: 0.99\n",
      "\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3180\n",
      "           1       0.99      0.99      0.99      3070\n",
      "\n",
      "    accuracy                           0.99      6250\n",
      "   macro avg       0.99      0.99      0.99      6250\n",
      "weighted avg       0.99      0.99      0.99      6250\n",
      "\n",
      "============================================================\n",
      "Treinando e avaliando o modelo: Logistic Regression\n",
      "Acurácia: 0.97\n",
      "Precisão: 0.98\n",
      "Recall: 0.96\n",
      "F1-Score: 0.97\n",
      "\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      3180\n",
      "           1       0.98      0.96      0.97      3070\n",
      "\n",
      "    accuracy                           0.97      6250\n",
      "   macro avg       0.97      0.97      0.97      6250\n",
      "weighted avg       0.97      0.97      0.97      6250\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas.oliveira\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Treinar e avaliar os modelos\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Treinando e avaliando o modelo: {model_name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Avaliar o desempenho do modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Exibir as métricas de avaliação\n",
    "    print(f\"Acurácia: {accuracy:.2f}\")\n",
    "    print(f\"Precisão: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n",
    "    print(\"\\nRelatório de Classificação:\\n\", report)\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
